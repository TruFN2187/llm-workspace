{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a4ad09",
   "metadata": {},
   "source": [
    "## Playground Description\n",
    "This notebook will probably always be a bit 'messy' as it is a place I use to troubleshoot and test new ideas that are still being refined. \n",
    "<br>\n",
    "<br>\n",
    "Once a concept has been tested to an acceptable point, I will try to transfer it to the 'component-lab' notebook which will be a semi-final testing ground for what I believe is ready to be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7004176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0921b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    'http://localhost:11434/api/generate',\n",
    "    json = {\n",
    "        'model': 'qwen3:8b',\n",
    "        'prompt': 'Can you explain Python Lambda functions?',\n",
    "        'stream': False,\n",
    "        'options': {\n",
    "            'num_ctx': 20000\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8025f9f",
   "metadata": {},
   "source": [
    "Run time for the 8b model was 3 min 40 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118defa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b69f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    'http://localhost:11434/api/generate',\n",
    "    json = {\n",
    "        'model': 'qwen3:14b',\n",
    "        'prompt': 'Can you explain Python Lambda functions?',\n",
    "        'stream': False,\n",
    "        'options': {\n",
    "            'num_ctx': 20000\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1969e93",
   "metadata": {},
   "source": [
    "trying to run the 14b model took 9 min 16 sec, perhaps I need to check the GPU driver on my laptop?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa1b58",
   "metadata": {},
   "source": [
    "Curious to see what the difference may be if I offload this to Coruscant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906116f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f72d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c16818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    'http://coruscant.local:11434/api/generate',\n",
    "    json = {\n",
    "        'model': 'qwen3:14b',\n",
    "        'prompt': 'Can you explain Python Lambda functions?',\n",
    "        'stream': False,\n",
    "        'options': {\n",
    "            'num_ctx': 20000\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0fe147",
   "metadata": {},
   "source": [
    "Well ok then, running on Coruscant returned 3x faster for the 14b model...\n",
    "Second run is about just as fast 3 min 48 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afbb226",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7054f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7301d7c9",
   "metadata": {},
   "source": [
    "The issue I was having with previous output was that the string needed to be printed by python first. By extracting it \"raw\" it was just a representation of the string with added characters. \n",
    "\n",
    "Curious to see how fast the 8b model performs on Coruscant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    'http://coruscant.local:11434/api/generate',\n",
    "    json = {\n",
    "        'model': 'qwen3:8b',\n",
    "        'prompt': 'Can you explain Python Lambda functions?',\n",
    "        'stream': False,\n",
    "        'options': {\n",
    "            'num_ctx': 20000\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abe1a09",
   "metadata": {},
   "source": [
    "First run... 32 seconds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ba594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt: str, model: str = 'qwen3:8b', ctx: int = 10000) -> str:\n",
    "    response = requests.post(\n",
    "    'http://coruscant.local:11434/api/generate',\n",
    "        json = {\n",
    "            'model': model,\n",
    "            'prompt': prompt,\n",
    "            'stream': False,\n",
    "            'options': {\n",
    "                'num_ctx': ctx\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    if 'response' not in data:\n",
    "        print('Unexpected output:', data)\n",
    "        return \"\"\n",
    "    \n",
    "    return data['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5620a827",
   "metadata": {},
   "source": [
    "From what I have seen it is possible to utilize the OpenAI method/syntax while still targeting ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4477f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://coruscant.local:11434/v1'\n",
    "\n",
    "ollama = OpenAI(base_url=base_url, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78fb7b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Decorators in Python are a powerful feature that allows you to modify or enhance functions, methods, or classes without altering their original code. They act as **wrappers** that add functionality to existing code, promoting modularity and separation of concerns. Here\\'s a structured explanation:\\n\\n---\\n\\n### **Key Concepts**\\n1. **Decorator Syntax**:\\n   - The `@` symbol is used to apply a decorator to a function or class.\\n   - Example:\\n     ```python\\n     @decorator_function\\n     def my_function():\\n         pass\\n     ```\\n   - This is equivalent to:\\n     ```python\\n     def my_function():\\n         pass\\n     my_function = decorator_function(my_function)\\n     ```\\n\\n2. **Decorator Structure**:\\n   - A decorator is typically a **function or class** that takes the target function as an argument and returns a **new function** that wraps the original.\\n   - Example:\\n     ```python\\n     def my_decorator(func):\\n         def wrapper():\\n             print(\"Before function call\")\\n             func()\\n             print(\"After function call\")\\n         return wrapper\\n     ```\\n\\n3. **Decorator Factory (With Parameters)**:\\n   - If a decorator needs to accept parameters, it requires an extra layer of nesting.\\n   - Example:\\n     ```python\\n     def repeat(num_times):\\n         def decorator(func):\\n             def wrapper(*args, **kwargs):\\n                 for _ in range(num_times):\\n                     func(*args, **kwargs)\\n             return wrapper\\n         return decorator\\n     ```\\n\\n4. **Class-Based Decorators**:\\n   - Classes can act as decorators by defining `__init__` and `__call__` methods.\\n   - Example:\\n     ```python\\n     class MyDecorator:\\n         def __init__(self, func):\\n             self.func = func\\n         def __call__(self, *args, **kwargs):\\n             print(\"Before function call\")\\n             result = self.func(*args, **kwargs)\\n             print(\"After function call\")\\n             return result\\n     ```\\n\\n---\\n\\n### **Common Use Cases**\\n- **Logging**:\\n  ```python\\n  @my_decorator\\n  def say_hello():\\n      print(\"Hello\")\\n  ```\\n- **Timing Execution**:\\n  ```python\\n  def timer(func):\\n      def wrapper(*args, **kwargs):\\n          start = time.time()\\n          result = func(*args, **kwargs)\\n          end = time.time()\\n          print(f\"Runtime: {end - start:.2f}s\")\\n          return result\\n      return wrapper\\n  ```\\n- **Access Control**:\\n  ```python\\n  def require_login(func):\\n      def wrapper(*args, **kwargs):\\n          if not is_authenticated():\\n              raise Exception(\"Login required\")\\n          return func(*args, **kwargs)\\n      return wrapper\\n  ```\\n\\n---\\n\\n### **Multiple Decorators**\\n- Decorators are applied **from bottom to top** (execution order is top to bottom, but application is bottom-up).\\n  ```python\\n  @decorator1\\n  @decorator2\\n  def func():\\n      pass\\n  ```\\n  - Equivalent to: `func = decorator1(decorator2(func))`\\n\\n---\\n\\n### **Built-in Decorators**\\n- `@property`: Converts a method into a readable attribute.\\n- `@staticmethod`: Defines a static method inside a class.\\n- `@classmethod`: Defines a class method that operates on the class rather than instances.\\n\\n---\\n\\n### **Summary**\\nDecorators are a flexible and reusable way to extend functionality in Python. They allow you to:\\n- Add behavior to functions or classes without modifying their code.\\n- Encapsulate common patterns (e.g., logging, caching).\\n- Use them with or without parameters, and in combination with other decorators.\\n\\nBy leveraging decorators, you can write cleaner, more maintainable code while keeping your logic modular.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model= 'qwen3:8b', messages= [{'role': 'user', 'content': 'What are decorators in Python?'}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d583721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decorators in Python are a powerful feature that allows you to modify or enhance functions, methods, or classes without altering their original code. They act as **wrappers** that add functionality to existing code, promoting modularity and separation of concerns. Here's a structured explanation:\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Concepts**\n",
      "1. **Decorator Syntax**:\n",
      "   - The `@` symbol is used to apply a decorator to a function or class.\n",
      "   - Example:\n",
      "     ```python\n",
      "     @decorator_function\n",
      "     def my_function():\n",
      "         pass\n",
      "     ```\n",
      "   - This is equivalent to:\n",
      "     ```python\n",
      "     def my_function():\n",
      "         pass\n",
      "     my_function = decorator_function(my_function)\n",
      "     ```\n",
      "\n",
      "2. **Decorator Structure**:\n",
      "   - A decorator is typically a **function or class** that takes the target function as an argument and returns a **new function** that wraps the original.\n",
      "   - Example:\n",
      "     ```python\n",
      "     def my_decorator(func):\n",
      "         def wrapper():\n",
      "             print(\"Before function call\")\n",
      "             func()\n",
      "             print(\"After function call\")\n",
      "         return wrapper\n",
      "     ```\n",
      "\n",
      "3. **Decorator Factory (With Parameters)**:\n",
      "   - If a decorator needs to accept parameters, it requires an extra layer of nesting.\n",
      "   - Example:\n",
      "     ```python\n",
      "     def repeat(num_times):\n",
      "         def decorator(func):\n",
      "             def wrapper(*args, **kwargs):\n",
      "                 for _ in range(num_times):\n",
      "                     func(*args, **kwargs)\n",
      "             return wrapper\n",
      "         return decorator\n",
      "     ```\n",
      "\n",
      "4. **Class-Based Decorators**:\n",
      "   - Classes can act as decorators by defining `__init__` and `__call__` methods.\n",
      "   - Example:\n",
      "     ```python\n",
      "     class MyDecorator:\n",
      "         def __init__(self, func):\n",
      "             self.func = func\n",
      "         def __call__(self, *args, **kwargs):\n",
      "             print(\"Before function call\")\n",
      "             result = self.func(*args, **kwargs)\n",
      "             print(\"After function call\")\n",
      "             return result\n",
      "     ```\n",
      "\n",
      "---\n",
      "\n",
      "### **Common Use Cases**\n",
      "- **Logging**:\n",
      "  ```python\n",
      "  @my_decorator\n",
      "  def say_hello():\n",
      "      print(\"Hello\")\n",
      "  ```\n",
      "- **Timing Execution**:\n",
      "  ```python\n",
      "  def timer(func):\n",
      "      def wrapper(*args, **kwargs):\n",
      "          start = time.time()\n",
      "          result = func(*args, **kwargs)\n",
      "          end = time.time()\n",
      "          print(f\"Runtime: {end - start:.2f}s\")\n",
      "          return result\n",
      "      return wrapper\n",
      "  ```\n",
      "- **Access Control**:\n",
      "  ```python\n",
      "  def require_login(func):\n",
      "      def wrapper(*args, **kwargs):\n",
      "          if not is_authenticated():\n",
      "              raise Exception(\"Login required\")\n",
      "          return func(*args, **kwargs)\n",
      "      return wrapper\n",
      "  ```\n",
      "\n",
      "---\n",
      "\n",
      "### **Multiple Decorators**\n",
      "- Decorators are applied **from bottom to top** (execution order is top to bottom, but application is bottom-up).\n",
      "  ```python\n",
      "  @decorator1\n",
      "  @decorator2\n",
      "  def func():\n",
      "      pass\n",
      "  ```\n",
      "  - Equivalent to: `func = decorator1(decorator2(func))`\n",
      "\n",
      "---\n",
      "\n",
      "### **Built-in Decorators**\n",
      "- `@property`: Converts a method into a readable attribute.\n",
      "- `@staticmethod`: Defines a static method inside a class.\n",
      "- `@classmethod`: Defines a class method that operates on the class rather than instances.\n",
      "\n",
      "---\n",
      "\n",
      "### **Summary**\n",
      "Decorators are a flexible and reusable way to extend functionality in Python. They allow you to:\n",
      "- Add behavior to functions or classes without modifying their code.\n",
      "- Encapsulate common patterns (e.g., logging, caching).\n",
      "- Use them with or without parameters, and in combination with other decorators.\n",
      "\n",
      "By leveraging decorators, you can write cleaner, more maintainable code while keeping your logic modular.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf73c1a",
   "metadata": {},
   "source": [
    "Working on the Wikipedia Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "790e29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34b67560",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_obj = wikipediaapi.Wikipedia(user_agent='TopicSearch (grant.mlanier@gmail.com)', language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1f6c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave = wiki_obj.page('The Great Wave off Kanagawa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7da0a8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5da7b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Great Wave off Kanagawa (Japanese: Á•ûÂ•àÂ∑ùÊ≤ñÊµ™Ë£è, Hepburn: Kanagawa-oki Nami Ura; lit.‚Äâ'Under the Wave off Kanagawa') is a woodblock print by the Japanese ukiyo-e artist Hokusai (1760‚Äì1849), created in late 1831 during the Edo period of Japanese history. The print depicts three boats moving through a storm-tossed sea, with a large, cresting wave forming a spiral in the centre over the boats and Mount Fuji in the background.\n",
      "The print is Hokusai's best-known work and the first in his series Thirty-six Views of Mount Fuji, in which the use of Prussian blue revolutionised Japanese prints. The composition of The Great Wave is a synthesis of traditional Japanese prints and use of graphical perspective developed in Europe, and earned him immediate success in Japan and later in Europe, where Hokusai's art inspired works by the Impressionists. Several museums throughout the world hold copies of The Great Wave, many of which came from 19th-century private collections of Japanese prints. Only about 100 prints, in varying conditions, are thought to have survived into the 21st century.\n",
      "The Great Wave off Kanagawa has been described as \"possibly the most reproduced image in the history of all art\", as well as being a contender for the \"most famous artwork in Japanese history\". This woodblock print has influenced several Western artists and musicians, including Claude Debussy, Vincent van Gogh and Claude Monet. Hokusai's younger colleagues Hiroshige and Utagawa Kuniyoshi were inspired to make their own wave-centric works.\n"
     ]
    }
   ],
   "source": [
    "print(wave.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cae3d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki(topic: str) -> str:\n",
    "    wiki_obj = wikipediaapi.Wikipedia(\n",
    "        user_agent='TopicSearch (grant.mlanier@gmail.com)',\n",
    "        language='en'\n",
    "    )\n",
    "    page = wiki_obj.page(topic)\n",
    "\n",
    "    return (\n",
    "        page.summary\n",
    "        if page.exists()\n",
    "        else f\"No page found on Wikipedia for {topic}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf7fb644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Great Wave off Kanagawa (Japanese: Á•ûÂ•àÂ∑ùÊ≤ñÊµ™Ë£è, Hepburn: Kanagawa-oki Nami Ura; lit.‚Äâ'Under the Wave off Kanagawa') is a woodblock print by the Japanese ukiyo-e artist Hokusai (1760‚Äì1849), created in late 1831 during the Edo period of Japanese history. The print depicts three boats moving through a storm-tossed sea, with a large, cresting wave forming a spiral in the centre over the boats and Mount Fuji in the background.\n",
      "The print is Hokusai's best-known work and the first in his series Thirty-six Views of Mount Fuji, in which the use of Prussian blue revolutionised Japanese prints. The composition of The Great Wave is a synthesis of traditional Japanese prints and use of graphical perspective developed in Europe, and earned him immediate success in Japan and later in Europe, where Hokusai's art inspired works by the Impressionists. Several museums throughout the world hold copies of The Great Wave, many of which came from 19th-century private collections of Japanese prints. Only about 100 prints, in varying conditions, are thought to have survived into the 21st century.\n",
      "The Great Wave off Kanagawa has been described as \"possibly the most reproduced image in the history of all art\", as well as being a contender for the \"most famous artwork in Japanese history\". This woodblock print has influenced several Western artists and musicians, including Claude Debussy, Vincent van Gogh and Claude Monet. Hokusai's younger colleagues Hiroshige and Utagawa Kuniyoshi were inspired to make their own wave-centric works.\n"
     ]
    }
   ],
   "source": [
    "print(get_wiki('The Great Wave off Kanagawa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "803de765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No page found on Wikipedia for asdgf\n"
     ]
    }
   ],
   "source": [
    "print(get_wiki('asdgf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ffb713",
   "metadata": {},
   "source": [
    "Trying to create tools to use for the LLM to use. Below is a sample of the syntax with the first example of searching Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ec532ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_wiki\",\n",
    "            \"description\": \"Fetch the summary for a topic from Wikipedia (or return a not-found message).\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"topic\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The topic to search Wikipedia for\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"topic\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "160ca709",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://coruscant.local:11434/v1'\n",
    "\n",
    "ollama = OpenAI(base_url=base_url, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "476485e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'what can you tell me about The Great Wave off Kanagawa?'\n",
    "    }\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(\n",
    "    model='qwen3:8b',\n",
    "    tools=tools,\n",
    "    messages= msg\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11383419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grant\\AppData\\Local\\Temp\\ipykernel_21644\\3202056457.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-292',\n",
       " 'choices': [{'finish_reason': 'tool_calls',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'message': {'content': '',\n",
       "    'refusal': None,\n",
       "    'role': 'assistant',\n",
       "    'annotations': None,\n",
       "    'audio': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': [{'id': 'call_e0m04a8z',\n",
       "      'function': {'arguments': '{\"topic\":\"The Great Wave off Kanagawa\"}',\n",
       "       'name': 'get_wiki'},\n",
       "      'type': 'function',\n",
       "      'index': 0}],\n",
       "    'reasoning': 'Okay, the user is asking about \"The Great Wave off Kanagawa.\" I remember that\\'s a famous woodblock print. Let me think. It\\'s by Hokusai, right? The title might be \"Fujiwara no Michikatsu\" or something similar. Wait, no, the correct title is \"The Great Wave off Kanagawa\" (Kanagawa Okino Fuj≈ç). The artwork depicts a massive wave with boats in the foreground and Mount Fuji in the background. It\\'s part of the Thirty-Six Views of Mount Fuji series. Hokusai created this in the Edo period. The wave is actually a tsunami, and the composition uses negative space effectively. The print is iconic in art history and has influenced many artists. I should check the Wikipedia summary to get accurate details. Let me call the get_wiki function with the topic.\\n'}}],\n",
       " 'created': 1769277031,\n",
       " 'model': 'qwen3:8b',\n",
       " 'object': 'chat.completion',\n",
       " 'service_tier': None,\n",
       " 'system_fingerprint': 'fp_ollama',\n",
       " 'usage': {'completion_tokens': 209,\n",
       "  'prompt_tokens': 164,\n",
       "  'total_tokens': 373,\n",
       "  'completion_tokens_details': None,\n",
       "  'prompt_tokens_details': None}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0487821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'what can you tell me about The Great Wave off Kanagawa?'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56182c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\"The Great Wave off Kanagawa\"** (Kanagawa Okinawamura / Kanagawa Okinawa-maru) is one of the most iconic and celebrated woodblock prints in the world. Here's a detailed overview:\n",
      "\n",
      "---\n",
      "\n",
      "### **Artist and Context**\n",
      "- **Created by**: **Katsushika Hokusai** (1760‚Äì1849), a master of **ukiyo-e** (Japanese woodblock print art), known for his intricate compositions and innovative use of perspective.\n",
      "- **Series**: Part of **\"Thirty-Six Views of Mount Fuji\"** (1831‚Äì1832), a collection of 46 prints celebrating the Japanese mountain. The wave is the fourth print in the series.\n",
      "- **Time Period**: Created during Japan's **Edo period** (1603‚Äì1868), a time of relative peace and cultural flourishing.\n",
      "\n",
      "---\n",
      "\n",
      "### **Artistic Description**\n",
      "- **Composition**: The print features a **dramatic, curling wave** (a stylized representation of a tsunami or storm) that looms over three small fishing boats. The wave's crest is rendered with **bold, swirling lines**, creating a sense of motion and power.\n",
      "- **Foreground**: The boats are small and distant, emphasizing the wave's dominance. A row of **plank houses** is visible in the distance, adding context to the coastal village.\n",
      "- **Background**: The **Mount Fuji** looms in the background, its snow-capped peak framed by the wave. The mountain is a symbol of Japan‚Äôs natural beauty and spiritual symbolism.\n",
      "- **Color**: The wave is rendered in **deep blue-gray**, achieved using the traditional **\"Ao\"** pigment, while the sky is a subtle grayish-blue. The contrast between the wave and the sky enhances the tension.\n",
      "\n",
      "---\n",
      "\n",
      "### **Artistic Techniques**\n",
      "- **Woodblock Printing**: Hokusai collaborated with artisans to carve the blocks, ink, and print the final piece. His attention to detail and mastery of **perspective** (blending traditional flatness with a sense of depth) make the wave feel almost alive.\n",
      "- **Dynamic Lines**: The wave‚Äôs curves and the movement of the ocean reflect Hokusai‚Äôs innovative approach, blending **musha-e** (warrior art) and **ukiyo-e** styles.\n",
      "\n",
      "---\n",
      "\n",
      "### **Cultural and Symbolic Significance**\n",
      "- **Nature‚Äôs Power**: The wave symbolizes the **unpredictability of nature** and Japan‚Äôs reverence for its landscapes. The contrast between the serene Mount Fuji and the stormy sea highlights themes of **balance and transience**.\n",
      "- **Impermanence**: The print aligns with **Zen Buddhist** ideas of impermanence, as the wave‚Äôs energy and the boats‚Äô fragility evoke the fleeting nature of life.\n",
      "- **Mythology**: The wave evokes **mythical imagery**, blending natural phenomena with a sense of the sublime.\n",
      "\n",
      "---\n",
      "\n",
      "### **Influence and Legacy**\n",
      "- **Western Art**: The print profoundly influenced **Impressionist** and **Post-Impressionist** artists like **Monet**, **van Gogh**, and **C√©zanne**, who admired its bold use of color and composition.\n",
      "- **Global Icon**: It has become an **international symbol of Japanese art and culture**, often used in pop culture, fashion, and design.\n",
      "- **Contemporary Relevance**: The wave represents both **beauty and chaos**, resonating with themes of resilience and nature‚Äôs power.\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Facts**\n",
      "- **Size**: Approx. 38 cm (width) √ó 27 cm (height).\n",
      "- **Technique**: Multi-woodblock printing with hand-painted details.\n",
      "- **Museum Holdings**: One of the original woodblocks is housed in the **Tokyo National Museum**, while many prints are in private collections or institutions like the **Metropolitan Museum of Art** (New York) and the **British Museum**.\n",
      "\n",
      "---\n",
      "\n",
      "This work remains a testament to Hokusai‚Äôs genius and the enduring appeal of Japan‚Äôs natural landscapes. Its dramatic interplay of form and symbolism continues to captivate audiences worldwide. üåä‚õ∞Ô∏è\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(\n",
    "    model='qwen3:8b',\n",
    "    messages= msg    \n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a136d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grant\\AppData\\Local\\Temp\\ipykernel_21644\\3202056457.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-664',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'message': {'content': '**\"The Great Wave off Kanagawa\"** (Kanagawa Okinawamura / Kanagawa Okinawa-maru) is one of the most iconic and celebrated woodblock prints in the world. Here\\'s a detailed overview:\\n\\n---\\n\\n### **Artist and Context**\\n- **Created by**: **Katsushika Hokusai** (1760‚Äì1849), a master of **ukiyo-e** (Japanese woodblock print art), known for his intricate compositions and innovative use of perspective.\\n- **Series**: Part of **\"Thirty-Six Views of Mount Fuji\"** (1831‚Äì1832), a collection of 46 prints celebrating the Japanese mountain. The wave is the fourth print in the series.\\n- **Time Period**: Created during Japan\\'s **Edo period** (1603‚Äì1868), a time of relative peace and cultural flourishing.\\n\\n---\\n\\n### **Artistic Description**\\n- **Composition**: The print features a **dramatic, curling wave** (a stylized representation of a tsunami or storm) that looms over three small fishing boats. The wave\\'s crest is rendered with **bold, swirling lines**, creating a sense of motion and power.\\n- **Foreground**: The boats are small and distant, emphasizing the wave\\'s dominance. A row of **plank houses** is visible in the distance, adding context to the coastal village.\\n- **Background**: The **Mount Fuji** looms in the background, its snow-capped peak framed by the wave. The mountain is a symbol of Japan‚Äôs natural beauty and spiritual symbolism.\\n- **Color**: The wave is rendered in **deep blue-gray**, achieved using the traditional **\"Ao\"** pigment, while the sky is a subtle grayish-blue. The contrast between the wave and the sky enhances the tension.\\n\\n---\\n\\n### **Artistic Techniques**\\n- **Woodblock Printing**: Hokusai collaborated with artisans to carve the blocks, ink, and print the final piece. His attention to detail and mastery of **perspective** (blending traditional flatness with a sense of depth) make the wave feel almost alive.\\n- **Dynamic Lines**: The wave‚Äôs curves and the movement of the ocean reflect Hokusai‚Äôs innovative approach, blending **musha-e** (warrior art) and **ukiyo-e** styles.\\n\\n---\\n\\n### **Cultural and Symbolic Significance**\\n- **Nature‚Äôs Power**: The wave symbolizes the **unpredictability of nature** and Japan‚Äôs reverence for its landscapes. The contrast between the serene Mount Fuji and the stormy sea highlights themes of **balance and transience**.\\n- **Impermanence**: The print aligns with **Zen Buddhist** ideas of impermanence, as the wave‚Äôs energy and the boats‚Äô fragility evoke the fleeting nature of life.\\n- **Mythology**: The wave evokes **mythical imagery**, blending natural phenomena with a sense of the sublime.\\n\\n---\\n\\n### **Influence and Legacy**\\n- **Western Art**: The print profoundly influenced **Impressionist** and **Post-Impressionist** artists like **Monet**, **van Gogh**, and **C√©zanne**, who admired its bold use of color and composition.\\n- **Global Icon**: It has become an **international symbol of Japanese art and culture**, often used in pop culture, fashion, and design.\\n- **Contemporary Relevance**: The wave represents both **beauty and chaos**, resonating with themes of resilience and nature‚Äôs power.\\n\\n---\\n\\n### **Key Facts**\\n- **Size**: Approx. 38 cm (width) √ó 27 cm (height).\\n- **Technique**: Multi-woodblock printing with hand-painted details.\\n- **Museum Holdings**: One of the original woodblocks is housed in the **Tokyo National Museum**, while many prints are in private collections or institutions like the **Metropolitan Museum of Art** (New York) and the **British Museum**.\\n\\n---\\n\\nThis work remains a testament to Hokusai‚Äôs genius and the enduring appeal of Japan‚Äôs natural landscapes. Its dramatic interplay of form and symbolism continues to captivate audiences worldwide. üåä‚õ∞Ô∏è',\n",
       "    'refusal': None,\n",
       "    'role': 'assistant',\n",
       "    'annotations': None,\n",
       "    'audio': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': None,\n",
       "    'reasoning': 'Okay, the user is asking about \"The Great Wave off Kanagawa.\" I need to start by confirming the correct title. Sometimes it\\'s called \"Great Wave\" or \"Wave off Kanagawa.\" The original Japanese name is \"Kanagawa Okinawamura\" or \"Kanagawa Okinawa-maru,\" which translates to \"Great Wave off Kanagawa.\" So I should mention that.\\n\\nFirst, I should mention the artist. It\\'s by Katsushika Hokusai, one of Japan\\'s most famous ukiyo-e artists. His style is known for combining traditional techniques with a sense of movement and perspective. Hokusai created this woodblock print in the early 19th century, around 1831. It\\'s part of the series \"Thirty-Six Views of Mount Fuji,\" which was published in 1831-1832. Wait, actually, the series is called \"Thirty-Six Views of Mount Fuji,\" so the wave is part of that series, not a standalone piece. That\\'s important to note.\\n\\nThe wave itself is a dramatic, swirling wave that seems to threaten the boats, which are in the foreground. The wave is portrayed as a giant, almost mythical creature. The composition is dynamic, with elements like the wave\\'s curves and the mountain in the background. The color is a blue-gray, which is the traditional Japanese blue called \"Ao.\" The use of perspective is interesting, blending a traditional flat style with a more three-dimensional approach. The wave is rendered with a sense of motion and realism, giving it a lifelike quality.\\n\\nHokusai was a key figure in the ukiyo-e movement, which typically focused on scenes from everyday life, theater, and landscapes. His work influenced Western art, especially the Impressionists, who admired his use of light and color. The print became an iconic image, and its famous composition is now a symbol of Japanese art and culture. The wave\\'s design has been used in various contexts, like art, design, and even as a symbol in pop culture, including video games and films. Also, in Japanese culture, the wave is sometimes associated with the idea of impermanence or the power of nature, which are common themes in traditional art and philosophy.\\n\\nI should also mention that the print is a woodblock print, which involves multiple artisans working together. The process includes carving the blocks, inking, and printing. Hokusai\\'s attention to detail and mastery of the technique contributed to the print\\'s enduring popularity. It\\'s important to note the historical context: the print was created during a time of relative peace in Japan, and it showcases the natural beauty and dramatic elements that were popular themes in Japanese art.\\n\\nAdditionally, since the user might be interested in broader implications, like the influence on Western art, I should highlight how the wave\\'s composition with its bold lines and dramatic use of space resonated with artists like Monet and van Gogh. Also, mentioning that the wave is often seen as both a natural wonder and a symbol of the unpredictability of nature could add depth.\\n\\nI need to structure this information clearly, starting with the basics: artist, title, series, creation date. Then describe the artwork\\'s elements, its artistic significance, cultural context, and its influence. Make sure to avoid any inaccuracies, like incorrect dates or misattributed styles. Also, check if there are any common misconceptions, such as the wave being an actual wave (it\\'s stylized) or the boats\\' position. It\\'s important to clarify that the wave is a stylized representation, not a realistic depiction.\\n'}}],\n",
       " 'created': 1769276171,\n",
       " 'model': 'qwen3:8b',\n",
       " 'object': 'chat.completion',\n",
       " 'service_tier': None,\n",
       " 'system_fingerprint': 'fp_ollama',\n",
       " 'usage': {'completion_tokens': 1611,\n",
       "  'prompt_tokens': 24,\n",
       "  'total_tokens': 1635,\n",
       "  'completion_tokens_details': None,\n",
       "  'prompt_tokens_details': None}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff252e3f",
   "metadata": {},
   "source": [
    "It looks like I was able to pass the tool to the model but it did not use it. Assuming this is becuase it was able to reason enough within its training data. I would need to find a way to prompt the LLM for something that it would have to pull from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5fd10826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"topic\":\"The Great Wave off Kanagawa\"}'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.tool_calls[0].function.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4bada812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The Great Wave off Kanagawa\" (Japanese: *Kanagawa-oki Nami Ura*) is a iconic woodblock print by the 19th-century Japanese artist Katsushika Hokusai (1760‚Äì1849), created in 1831 during Japan‚Äôs Edo period. It is the first print in Hokusai‚Äôs series *Thirty-six Views of Mount Fuji* and showcases his mastery of ukiyo-e (woodblock printing).  \n",
      "\n",
      "The composition features a towering, curling wave‚Äîsymbolizing the raw power of nature‚Äîthreatening three boats adrift in a stormy sea. In the background, the serene silhouette of Mount Fuji contrasts with the chaotic waves, embodying the harmony between natural elements. Hokusai‚Äôs use of **Prussian blue** (a newly introduced pigment) and European-inspired linear perspective revolutionized Japanese printmaking, blending traditional Japanese aesthetics with global artistic trends.  \n",
      "\n",
      "This print became a cultural phenomenon, inspiring Western artists like **Vincent van Gogh**, **Claude Monet**, and **Claude Debussy**, who referenced it in their works. Today, it is one of the most reproduced and recognized artworks globally, with only around 100 original prints surviving. Its enduring legacy lies in its ability to capture both the sublime beauty and ferocity of nature, making it a symbol of Japanese artistry and a bridge between Eastern and Western artistic traditions.\n"
     ]
    }
   ],
   "source": [
    "for tool_call in response.choices[0].message.tool_calls:\n",
    "    if tool_call.function.name == 'get_wiki':\n",
    "        args = json.loads(tool_call.function.arguments)\n",
    "        topic = get_wiki(args['topic'])\n",
    "\n",
    "        msg.append(\n",
    "            {\n",
    "                'role': 'tool',\n",
    "                'tool_call_id': tool_call.id,\n",
    "                'content': json.dumps({'topic': topic})\n",
    "            }\n",
    "        )\n",
    "\n",
    "response = ollama.chat.completions.create(\n",
    "    model='qwen3:8b',\n",
    "    messages=msg,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "02a224fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Section: Pre-1600 (2):\n",
       "41 ‚Äì Claudius is proclaimed Roman emperor by the Praetorian Guard after they assassinate the previous emperor, his nephew Caligula.\n",
       "914 ‚Äì Start of the First Fatimid invasion of Egypt.\n",
       "1438 ‚Äì The Council of Basel suspends Pope Eugene IV.\n",
       "1458 ‚Äì Matthias Corvinus is elected King of Hungary.\n",
       "1536 ‚Äì King Henry VIII of England suffers an accident while jousting, leading to a brain injury that historians say may have influenced his later erratic behaviour and possible impotence.\n",
       "Subsections (0):"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = wiki_obj.page('january_24')\n",
    "\n",
    "today_secs = today.sections\n",
    "# type(today_secs[0])\n",
    "today_secs[0].section_by_title('Pre-1600')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8699041a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events\n",
      "Births\n",
      "Deaths\n",
      "Holidays and observances\n",
      "References\n",
      "External links\n"
     ]
    }
   ],
   "source": [
    "for sec in today_secs:\n",
    "    print(sec.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f17af5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 ‚Äì Claudius is proclaimed Roman emperor by the Praetorian Guard after they assassinate the previous emperor, his nephew Caligula.\n",
      "914 ‚Äì Start of the First Fatimid invasion of Egypt.\n",
      "1438 ‚Äì The Council of Basel suspends Pope Eugene IV.\n",
      "1458 ‚Äì Matthias Corvinus is elected King of Hungary.\n",
      "1536 ‚Äì King Henry VIII of England suffers an accident while jousting, leading to a brain injury that historians say may have influenced his later erratic behaviour and possible impotence.\n",
      "\n",
      "\n",
      "1651 ‚Äì Arauco War: Spanish and Mapuche authorities meet in the Parliament of Boroa renewing the fragile peace established at the parliaments of Quill√≠n in 1641 and 1647.\n",
      "1679 ‚Äì King Charles II of England dissolves the Cavalier Parliament.\n",
      "1742 ‚Äì Charles VII Albert becomes Holy Roman Emperor.\n",
      "1758 ‚Äì During the Seven Years' War the leading burghers of K√∂nigsberg submit to Elizabeth of Russia, thus forming Russian Prussia (until 1763).\n",
      "1817 ‚Äì Crossing of the Andes: Many soldiers of Juan Gregorio de las Heras are captured during the action of Picheuta.\n",
      "1835 ‚Äì Slaves in Salvador da Bahia, Brazil, stage a revolt, which is instrumental in ending slavery there 50 years later.\n",
      "1848 ‚Äì California Gold Rush: James W. Marshall finds gold at Sutter's Mill near Sacramento.\n",
      "1857 ‚Äì The University of Calcutta is formally founded as the first fully fledged university in South Asia.\n",
      "1859 ‚Äì The United Principalities of Moldavia and Wallachia (later named Romania) is formed as a personal union under the rule of Domnitor Alexandru Ioan Cuza.\n",
      "1900 ‚Äì Second Boer War: Boers stop a British attempt to break the Siege of Ladysmith in the Battle of Spion Kop.\n",
      "\n",
      "\n",
      "1908 ‚Äì The first Boy Scout troop is organized in England by Robert Baden-Powell.\n",
      "1915 ‚Äì World War I: British Grand Fleet battle cruisers under Vice-Admiral Sir David Beatty engage Rear-Admiral Franz von Hipper's battle cruisers in the Battle of Dogger Bank.\n",
      "1916 ‚Äì In Brushaber v. Union Pacific Railroad Co., the Supreme Court of the United States declares the federal income tax constitutional.\n",
      "1918 ‚Äì The Gregorian calendar is introduced in Russia by decree of the Council of People's Commissars effective February 14 (New Style).\n",
      "1933 ‚Äì The 20th Amendment to the United States Constitution is ratified, changing the beginning and end of terms for all elected federal offices.\n",
      "1935 ‚Äì Gottfried Krueger Brewing Company starts selling the first canned beer.\n",
      "1939 ‚Äì The deadliest earthquake in Chilean history strikes Chill√°n, killing approximately 28,000 people.\n",
      "1942 ‚Äì World War II: The Allies bombard Bangkok, leading Thailand, then under Japanese control, to declare war against the United States and United Kingdom.\n",
      "1943 ‚Äì World War II: Franklin D. Roosevelt and Winston Churchill conclude a conference in Casablanca.\n",
      "1946 ‚Äì The United Nations General Assembly passes its first resolution to establish the United Nations Atomic Energy Commission.\n",
      "1960 ‚Äì Algerian War: Some units of European volunteers in Algiers stage an insurrection known as the \"barricades week\", during which they seize government buildings and clash with local police.\n",
      "1961 ‚Äì Goldsboro B-52 crash: A bomber carrying two H-bombs breaks up in mid-air over North Carolina. The uranium core of one weapon remains lost.\n",
      "1966 ‚Äì Air India Flight 101 crashes into Mont Blanc.\n",
      "1968 ‚Äì Vietnam War: The 1st Australian Task Force launches Operation Coburg against the North Vietnamese Army and Viet Cong during wider fighting around Long B√¨nh and Bi√™n H√≤a.\n",
      "1972 ‚Äì Japanese Sgt. Shoichi Yokoi is found hiding in a Guam jungle, where he had been since the end of World War II.\n",
      "1977 ‚Äì The Atocha massacre occurs in Madrid during the Spanish transition to democracy.\n",
      "1978 ‚Äì Soviet satellite Kosmos 954, with a nuclear reactor on board, burns up in Earth's atmosphere, scattering radioactive debris over Canada's Northwest Territories. Only 1% is recovered.\n",
      "1984 ‚Äì Apple Computer places the Macintosh personal computer on sale in the United States.\n",
      "1986 ‚Äì The Voyager 2 space probe makes its closest approach to Uranus.\n",
      "1987 ‚Äì About 20,000 protestors march in a civil rights demonstration in Forsyth County, Georgia, United States.\n",
      "1988 ‚Äì An Aeroflot Yakovlev Yak-40 crashes at Nizhnevartovsk Airport, killing 27 people.\n",
      "1989 ‚Äì Notorious serial killer Ted Bundy, with over 30 known victims, is executed in the electric chair at the Florida State Prison.\n",
      "1990 ‚Äì Japan launches Hiten, the country's first lunar probe, the first robotic lunar probe since the Soviet Union's Luna 24 in 1976, and the first lunar probe launched by a country other than Soviet Union or the United States.\n",
      "2003 ‚Äì The United States Department of Homeland Security officially begins operation.\n",
      "2009 ‚Äì Cyclone Klaus makes landfall near Bordeaux, France, causing 26 deaths as well as extensive disruptions to public transport and power supplies.\n",
      "2011 ‚Äì At least 35 are killed and 180 injured in a bombing at Moscow's Domodedovo Airport.\n",
      "2018 ‚Äì Former doctor Larry Nassar is sentenced up to 175 years in prison after being found guilty of using his position to sexually abuse female gymnasts.\n",
      "2024 ‚Äì An Ilyushin Il-76 crashes in Russia's Korochansky District, killing all 74 people on board.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sec in today_secs[0].sections:\n",
    "    print(sec.text + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d1868107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['41 ‚Äì Claudius is proclaimed Roman emperor by the Praetorian Guard after they assassinate the previous emperor, his nephew Caligula.',\n",
       " '914 ‚Äì Start of the First Fatimid invasion of Egypt.',\n",
       " '1438 ‚Äì The Council of Basel suspends Pope Eugene IV.',\n",
       " '1458 ‚Äì Matthias Corvinus is elected King of Hungary.',\n",
       " '1536 ‚Äì King Henry VIII of England suffers an accident while jousting, leading to a brain injury that historians say may have influenced his later erratic behaviour and possible impotence.']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today_secs[0].sections[0].text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bc9bc497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Portal:Current_events'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_obj = wikipediaapi.Wikipedia(user_agent='TopicSearch (grant.mlanier@gmail.com)', language='en')\n",
    "cur_event = wiki_obj.page('Portal:Current events')\n",
    "\n",
    "# cur_event.exists()\n",
    "print(cur_event.text)\n",
    "cur_event.fullurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "73da289a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cur_event.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "74772961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.title for s in cur_event.sections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ac5c93d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "https://en.wikipedia.org/wiki/Portal:Current_events\n",
      "summary len: 0\n",
      "text len: 0\n",
      "sections: []\n"
     ]
    }
   ],
   "source": [
    "print(cur_event.exists())\n",
    "print(cur_event.fullurl)\n",
    "print(\"summary len:\", len(cur_event.summary or \"\"))\n",
    "print(\"text len:\", len(cur_event.text or \"\"))\n",
    "print(\"sections:\", [s.title for s in cur_event.sections][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186da754",
   "metadata": {},
   "source": [
    "It appears that the 'Current Events' page does not return a value using the wikipedia api because of some issue with templates/transclusions(?) The suggestion is to use a direct call to the MediaWiki API using requests and bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e0bbe5",
   "metadata": {},
   "source": [
    "### Trying to build out more organized 'class' structure / algin more with OOP principals  \n",
    "Going through some of the udemy course, most start off using just a simple 'function' approach while teaching, but (in at least one of them) switches to using 'class' or OOP focused methods. This, based on my research, should help reduce some of the repetitive nature of things like making the call to the the LLM client. This will be the first class that I work on with the idea that any agent that follows would end up relying on it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5bfc82",
   "metadata": {},
   "source": [
    "The following is the product of working through some of the above code with GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31525ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import Iterable, Optional\n",
    "from IPython.display import Markdown, display # for displaying formatted text in Jupyter\n",
    "import json # added after moving the tool agent to be 'chat focused'\n",
    "import requests # added to test simple tool calls\n",
    "from dotenv import load_dotenv # added to protect API keys\n",
    "import os\n",
    "import wikipediaapi # needed for the get_wiki tool\n",
    "\n",
    "load_dotenv(override=True)\n",
    "open_weather_api_key = os.getenv('OPEN_WEATHER_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a09c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallLlmClient:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_url: str,\n",
    "        model: str,\n",
    "        api_key: str = \"ollama\",\n",
    "        default_tools: Optional[list] = None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.default_tools = default_tools or []\n",
    "\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=api_key,\n",
    "        )\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        messages: list[dict],\n",
    "        tools: Optional[list] = None,\n",
    "        **kwargs,\n",
    "    ) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            tools=tools or self.default_tools,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e0529",
   "metadata": {},
   "source": [
    "next this is a simple example of its usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ffc01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The artist of *The Great Wave off Kanagawa* (also known as *The Great Wave* or *Kanagawa Kani* in Japanese) is **Katsushika Hokusai** (1769‚Äì1849). This iconic woodblock print is part of his series *Thirty-Six Views of Mount Fuji*, created in the 1830s. Hokusai was a renowned Japanese ukiyo-e (woodblock print) artist from the Edo period, and this work remains one of the most celebrated and recognizable images in world art."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "msg = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Who was the artist of The Great Wave off Kanagawa?'\n",
    "    }\n",
    "]\n",
    "\n",
    "llm = CallLlmClient(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    model=\"qwen3:8b\",\n",
    ")\n",
    "\n",
    "reply = llm.chat(msg)\n",
    "display(Markdown(reply))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156a6ef",
   "metadata": {},
   "source": [
    "As I was working through some questions about building agents, GPT called out the above LLM class is not really setup to handle tool calls (despite it taking it as a parameter in the init method??) \n",
    "<br>\n",
    "<br>\n",
    "From what I can see the change it recommends would return the raw 'message' object instead of 'message.content' like the original does. This is needed to determine if the first call/message returned a tool call it seems so in the case that it does it can then be leveraged instead of just returning the initial output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acccc9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edited class that allows Agent to determine what to do with message\n",
    "class LlmClient:\n",
    "    def __init__(self, base_url: str, model: str, api_key: str = \"ollama\"):\n",
    "        self.model = model\n",
    "        self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        messages: list[dict],\n",
    "        tools: Optional[list] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a38555",
   "metadata": {},
   "source": [
    "now with this class we also set up the Base Agent class to make sure the agents return the output. Then specialized agent classes would be made to handle the message differently depending on the situation and then manipulate things further before output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b96a3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    def __init__(self, llm: LlmClient, system_prompt: str):\n",
    "        self.llm = llm\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def build_messages(self, user_input: str) -> list[dict]:\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ]\n",
    "\n",
    "    def run(self, user_input: str) -> str:\n",
    "        message = self.llm.chat(self.build_messages(user_input))\n",
    "        return message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b095ca8",
   "metadata": {},
   "source": [
    "This next class is a demo of a class GPT made that would utilize the tools if they are relevant and called upon it seems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "150d6a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolAgent(BaseAgent):\n",
    "    def __init__(self, llm: LlmClient, tools: list):\n",
    "        super().__init__(\n",
    "            llm,\n",
    "            system_prompt=\"You may call tools when appropriate.\",\n",
    "        )\n",
    "        self.tools = tools\n",
    "\n",
    "    def run(self, user_input: str):\n",
    "        message = self.llm.chat(\n",
    "            messages=self.build_messages(user_input),\n",
    "            tools=self.tools,\n",
    "        )\n",
    "\n",
    "        # Model wants to call a tool\n",
    "        if message.tool_calls:\n",
    "            return message  # or dispatch tool execution\n",
    "\n",
    "        return message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230fa719",
   "metadata": {},
   "source": [
    "Looking over things though, it looks like this ToolAgent class is not actually set up to handle the tools (which I guess makes sense seeing as I have not exposed any to it yet) and GPT even calls out that it can help build a 'execution loop' which would allow the tool agent to be utilized it seems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396f74d",
   "metadata": {},
   "source": [
    "This following class is redesigned to handle a message history and be more 'chat' focused as well. Speaking with GPT about the history, it recommends keeping the history's 'state' within the agent itself instead of relating it back to the LLMClient. This way new agents will not be impacted by previous agents messages that may cause issues. (like additional tool calls?)\n",
    "> I am curious though as to how (if needed) I would be able to break off a section of the history to pass to a different agent if needed. It is a structured object so there should be some way I would think but may need to get creative in how I pull the index or value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7693234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatToolAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: \"LlmClient\",\n",
    "        tools: list,\n",
    "        tool_dispatch: dict,\n",
    "        system_prompt: str = \"You are a helpful assistant. Use tools when helpful.\",\n",
    "        max_steps: int = 8,\n",
    "        max_history_msgs: Optional[int] = 50,\n",
    "    ):\n",
    "        self.llm = llm\n",
    "        self.tools = tools\n",
    "        self.tool_dispatch = tool_dispatch\n",
    "        self.system_prompt = system_prompt\n",
    "        self.max_steps = max_steps\n",
    "        self.max_history_msgs = max_history_msgs\n",
    "\n",
    "        # Conversation state lives here\n",
    "        self.messages: list[dict] = []\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Start a fresh conversation.\"\"\"\n",
    "        self.messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "\n",
    "    def _trim_history(self):\n",
    "        \"\"\"\n",
    "        Keep system + last N messages.\n",
    "        (This is a simple heuristic; later you can summarize older turns.)\n",
    "        \"\"\"\n",
    "        if self.max_history_msgs is None:\n",
    "            return\n",
    "        # always keep the system message at index 0\n",
    "        if len(self.messages) > (1 + self.max_history_msgs):\n",
    "            self.messages = [self.messages[0]] + self.messages[-self.max_history_msgs :]\n",
    "\n",
    "    def ask(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        One user turn, but may require multiple internal steps if tools are called.\n",
    "        Conversation history is preserved across calls.\n",
    "        \"\"\"\n",
    "        # Add user message to ongoing history\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self._trim_history()\n",
    "\n",
    "        for _ in range(self.max_steps):\n",
    "            # 1) model call\n",
    "            message = self.llm.chat(messages=self.messages, tools=self.tools)\n",
    "\n",
    "            # append assistant message to history\n",
    "            self.messages.append(message.model_dump())\n",
    "            self._trim_history()\n",
    "\n",
    "            # 2) if no tool calls -> return assistant content\n",
    "            if not message.tool_calls:\n",
    "                return message.content or \"\"\n",
    "\n",
    "            # 3) execute each requested tool call\n",
    "            for call in message.tool_calls:\n",
    "                tool_name = call.function.name\n",
    "                raw_args = call.function.arguments or \"{}\"\n",
    "\n",
    "                try:\n",
    "                    args = json.loads(raw_args)\n",
    "                except json.JSONDecodeError:\n",
    "                    result = f\"ERROR: Invalid JSON arguments: {raw_args}\"\n",
    "                else:\n",
    "                    func = self.tool_dispatch.get(tool_name)\n",
    "                    if not func:\n",
    "                        result = f\"ERROR: Unknown tool '{tool_name}'\"\n",
    "                    else:\n",
    "                        try:\n",
    "                            result = func(**args)\n",
    "                        except Exception as e:\n",
    "                            result = f\"ERROR: Tool '{tool_name}' failed: {e}\"\n",
    "\n",
    "                # 4) feed tool result back\n",
    "                self.messages.append(\n",
    "                    {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": call.id,\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": str(result),\n",
    "                    }\n",
    "                )\n",
    "                self._trim_history()\n",
    "\n",
    "        return \"ERROR: Reached max tool steps without a final answer.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2a142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Hello, How are you?'\n",
    "    }\n",
    "]\n",
    "\n",
    "llm = LlmClient(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    model=\"qwen3:8b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f52fd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.chat(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1fe47f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help! How are you today? üòä\",\n",
       " 'refusal': None,\n",
       " 'role': 'assistant',\n",
       " 'annotations': None,\n",
       " 'audio': None,\n",
       " 'function_call': None,\n",
       " 'tool_calls': None,\n",
       " 'reasoning': 'Okay, the user asked \"Hello, How are you?\" I need to respond appropriately. Since I\\'m an AI, I don\\'t have feelings, so I should mention that I don\\'t have emotions. But I should also be friendly and ask how they\\'re doing. Let me check if there\\'s any specific context I should consider. The user might just be greeting me, so a simple, cheerful response should work. I\\'ll make sure to keep it conversational and open-ended in case they have more to say.\\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(response)\n",
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "227719ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help! How are you today? üòä\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc5ab091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.types.chat.chat_completion_message.ChatCompletionMessage"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d801499e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60865977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(msg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f11ecdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['user', 'Hello, How are you?'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg[0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "812cc73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't have access to real-time weather data, so I can't provide current conditions for Hammond, Louisiana. However, here's a general overview of the typical weather in the area:\n",
       "\n",
       "### **Typical Weather in Hammond, Louisiana:**\n",
       "- **Summer (June‚ÄìAugust):**  \n",
       "  - **Highs:** 90¬∞F (32¬∞C) or warmer.  \n",
       "  - **Lows:** 75¬∞F (24¬∞C) or higher.  \n",
       "  - **Conditions:** Humid, with frequent thunderstorms and heavy rainfall, especially in July and August.  \n",
       "\n",
       "- **Winter (December‚ÄìFebruary):**  \n",
       "  - **Highs:** 65¬∞F (18¬∞C) to 75¬∞F (24¬∞C).  \n",
       "  - **Lows:** 50¬∞F (10¬∞C) to 55¬∞F (13¬∞C).  \n",
       "  - **Conditions:** Mild, with occasional rain or light snow.  \n",
       "\n",
       "- **Spring (March‚ÄìMay) & Fall (September‚ÄìNovember):**  \n",
       "  - **Highs:** 75¬∞F (24¬∞C) to 85¬∞F (29¬∞C).  \n",
       "  - **Lows:** 60¬∞F (16¬∞C) to 70¬∞F (21¬∞C).  \n",
       "  - **Conditions:** Variable, with warm days and cooler evenings. Spring often brings thunderstorms, while fall can have cooler temperatures and occasional rain.  \n",
       "\n",
       "### **Tips for Real-Time Updates:**  \n",
       "For current conditions, check services like:  \n",
       "- **National Weather Service (NWS):** [www.weather.gov](https://www.weather.gov)  \n",
       "- **AccuWeather** or **The Weather Channel** apps.  \n",
       "- **Local news stations** (e.g., KALB, KPLC, or KATC).  \n",
       "\n",
       "Let me know if you‚Äôd like help finding a specific resource! üå§Ô∏è"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "msg = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Can you tell me the weather in Hammond Louisiana?'\n",
    "    }\n",
    "]\n",
    "response = llm.chat(msg)\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d91b7c6",
   "metadata": {},
   "source": [
    "The code in the cell above shows that the default model does not have access to real time data (sanity check) Below I used GPT to write up a quick weather checking function, to test out the tool agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37574c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "api_key = open_weather_api_key\n",
    "location = \"Hammond,Louisiana,US\"\n",
    "params = {\n",
    "        \"q\": location,\n",
    "        \"appid\": api_key,\n",
    "        \"units\": \"imperial\"  # Fahrenheit\n",
    "    }\n",
    "\n",
    "response = requests.get(url, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afa89e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coord': {'lon': -90.4612, 'lat': 30.5044},\n",
       " 'weather': [{'id': 803,\n",
       "   'main': 'Clouds',\n",
       "   'description': 'broken clouds',\n",
       "   'icon': '04d'}],\n",
       " 'base': 'stations',\n",
       " 'main': {'temp': 57.2,\n",
       "  'feels_like': 55.35,\n",
       "  'temp_min': 57.2,\n",
       "  'temp_max': 57.2,\n",
       "  'pressure': 1018,\n",
       "  'humidity': 58,\n",
       "  'sea_level': 1018,\n",
       "  'grnd_level': 1018},\n",
       " 'visibility': 10000,\n",
       " 'wind': {'speed': 10.36, 'deg': 350},\n",
       " 'clouds': {'all': 75},\n",
       " 'dt': 1769804506,\n",
       " 'sys': {'type': 1,\n",
       "  'id': 4342,\n",
       "  'country': 'US',\n",
       "  'sunrise': 1769777644,\n",
       "  'sunset': 1769816164},\n",
       " 'timezone': -21600,\n",
       " 'id': 4326868,\n",
       " 'name': 'Hammond',\n",
       " 'cod': 200}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30342d18",
   "metadata": {},
   "source": [
    "Ran into an issue where open weather's api was not showing the key as valid, says it can take up to 2 hours to show up.. trying something else..\n",
    "> coming back to it the API is working now so looks like I have two methods to pull weather data now. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ee1218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddccb32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temp': 57.2,\n",
       " 'feels_like': 55.35,\n",
       " 'temp_min': 57.2,\n",
       " 'temp_max': 57.2,\n",
       " 'pressure': 1018,\n",
       " 'humidity': 58,\n",
       " 'sea_level': 1018,\n",
       " 'grnd_level': 1018}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd587de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['main']['temp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07e66a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(city: str, state: str = None, api_key:str = open_weather_api_key) -> dict:\n",
    "    if not api_key:\n",
    "        return {\"error\": \"No API key provided.\"}\n",
    "    \n",
    "    location = f\"{city},{state},US\" if state else f\"{city},US\"\n",
    "    url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "    params = { \n",
    "        \"q\": location,\n",
    "        \"appid\": api_key,\n",
    "        \"units\": \"imperial\"  # Fahrenheit\n",
    "    }\n",
    "   \n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    return {\n",
    "        \"city\": data[\"name\"],\n",
    "        \"temperature_f\": data[\"main\"][\"temp\"],\n",
    "        \"feels_like_f\": data[\"main\"][\"feels_like\"],\n",
    "        \"humidity\": data[\"main\"][\"humidity\"],\n",
    "        \"conditions\": data[\"weather\"][0][\"description\"],\n",
    "        \"wind_mph\": data[\"wind\"][\"speed\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2eaa11",
   "metadata": {},
   "source": [
    "Moving the `get_wiki` function down for ease fo viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6befeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki(topic: str) -> str:\n",
    "    wiki_obj = wikipediaapi.Wikipedia(\n",
    "        user_agent='TopicSearch (grant.mlanier@gmail.com)',\n",
    "        language='en'\n",
    "    )\n",
    "    page = wiki_obj.page(topic)\n",
    "\n",
    "    return (\n",
    "        page.summary\n",
    "        if page.exists()\n",
    "        else f\"No page found on Wikipedia for {topic}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cf9251",
   "metadata": {},
   "source": [
    "now we create the tools list. I noticed that the two udemy courses use slightly different methods. Where the first one did it all in one go, the second formats the inner function call first then passes that to a smaller format of the overall. The second method looks like it could be used to help keep things compact but will look into that later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6008c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_wiki\",\n",
    "            \"description\": \"Fetch the summary for a topic from Wikipedia (or return a not-found message).\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"topic\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The topic to search Wikipedia for\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"topic\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather for a specified city (and optional state) in the US.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city to get the weather for\"\n",
    "                    },\n",
    "                    \"state\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The state abbreviation (optional)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"city\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23a0abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_dispatch = {\n",
    "    \"get_wiki\": get_wiki,\n",
    "    \"get_weather\": get_weather,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94fd3b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_agent = ChatToolAgent(llm, tools, tools_dispatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1f28387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The current weather in Hammond, LA remains:  \n",
       "- **Temperature**: 59¬∞F (feels like 56.66¬∞F)  \n",
       "- **Humidity**: 44%  \n",
       "- **Conditions**: Broken clouds  \n",
       "- **Wind**: 10.36 mph  \n",
       "\n",
       "Let me know if you'd like updates or additional details! üòä"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gen_agent.ask('Can you tell me the weather in Hammond Louisiana?')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79bb1f",
   "metadata": {},
   "source": [
    "After a bit of back and forth to confirm the process I would want to use to call the functions, it looks like things are working well with the weather tool but I am curious to see if the wiki tool makes an actual impact. Below is a 'raw' call to the llm asking it about the town I live in, I will then pass that to the tool agent to compare if it was able to figure if it should check wiki or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0d38e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hammond, Louisiana, is a city located in **St. Tammany Parish**, part of the **greater New Orleans metropolitan area** (approximately 35 miles northwest of New Orleans). Here‚Äôs an overview of the city:\n",
       "\n",
       "---\n",
       "\n",
       "### **Key Details**\n",
       "- **Population**: Approximately **28,000** (as of recent estimates).\n",
       "- **Founded**: Incorporated in the **1950s** as a planned suburban community.\n",
       "- **Location**: Situated along the **Mississippi River** and near the **Gulf Coast**, with access to the **Bayou La Batre** area.\n",
       "\n",
       "---\n",
       "\n",
       "### **Economy & Geography**\n",
       "- **Economic Base**: Primarily residential and commercial development, including retail, healthcare, and light industry.\n",
       "- **Geography**: The city sits in a flood-prone region due to its low-lying terrain and proximity to the Mississippi River. It is part of the **New Orleans Flood Protection System**, which includes levees and floodwalls.\n",
       "- **Proximity**: Adjacent to **Slidell**, **Mandeville**, and **Metairie**, making it a suburban hub with easy access to New Orleans.\n",
       "\n",
       "---\n",
       "\n",
       "### **Culture & Community**\n",
       "- **Recreational Areas**: \n",
       "  - **St. Tammany Parish Park** (offers boating, fishing, and camping).\n",
       "  - **Bayou La Batre** (a popular freshwater lake for recreation).\n",
       "  - **Hammocks Creek** and **Pine Lake** for outdoor activities.\n",
       "- **Cultural Attractions**: \n",
       "  - The **Hammond Historical Society** and local museums.\n",
       "  - Annual events like the **Hammond Harvest Festival** (celebrating agriculture and community).\n",
       "- **Local Flavor**: Known for its blend of Southern hospitality, Creole influence, and family-friendly atmosphere.\n",
       "\n",
       "---\n",
       "\n",
       "### **Challenges**\n",
       "- **Flood Risk**: Vulnerable to hurricanes, heavy rainfall, and river flooding (e.g., during Hurricane Katrina in 2005).\n",
       "- **Environmental Issues**: Ongoing concerns about coastal erosion and water management in the region.\n",
       "\n",
       "---\n",
       "\n",
       "### **Notable Fact**\n",
       "- **Hammond, Louisiana**, is distinct from **Hammond, Indiana**, and the **Hammond, Texas** area. The Louisiana city is part of the **New Orleans metropolitan statistical area**, while others are in different states.\n",
       "\n",
       "---\n",
       "\n",
       "If you‚Äôre looking for specific details (e.g., schools, businesses, or historical landmarks), feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "msg = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'What can you tell me about Hammond Louisiana?'\n",
    "    }\n",
    "]\n",
    "response = llm.chat(msg)\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0f47d",
   "metadata": {},
   "source": [
    "It says Hammond is in St Landry Parish???? That is clearly wrong, but I wonder if that is just this attempt, and future attempts would be better? (run time is around 25-30 seconds)\n",
    "> - Second attempt said it was in Ascension Parish.....\n",
    "> - At least this confirms that future attempts would be different but how often does it get it right? vs will the tool call be more accurate.\n",
    "> - third call said st tammany....\n",
    "Lets try the tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8aca3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It seems there's a technical limitation in accessing external resources like Wikipedia. However, here's a summary of **Hammond, Louisiana** based on available knowledge:\n",
       "\n",
       "### **Hammond, Louisiana**  \n",
       "- **Location**: A city in Jefferson Parish, approximately 10 miles north of New Orleans.  \n",
       "- **History**:  \n",
       "  - Founded in 1836, it was a key stop on the Natchez Trace Parkway.  \n",
       "  - Known as the \"Cradle of the Blues\" for its jazz and music heritage.  \n",
       "  - Played a role in the Civil War, including the Battle of Hammond (1862).  \n",
       "- **Notable Landmarks**:  \n",
       "  - **Jefferson Parish Courthouse** (built in 1852, listed on the National Register of Historic Places).  \n",
       "  - **Hammond Park** (a historic district with antebellum homes).  \n",
       "  - **St. Louis Cathedral** (part of the New Orleans metropolitan area).  \n",
       "- **Culture**: Rich in Creole and French heritage, with vibrant music scene and proximity to cultural hubs like New Orleans.  \n",
       "- **Population**: ~50,000 (as of recent estimates).  \n",
       "\n",
       "Let me know if you'd like details about specific landmarks, history, or local culture! üòä"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gen_agent.ask('What can you tell me about Hammond Louisiana?')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c9d77",
   "metadata": {},
   "source": [
    "So it looks like it failed but the cool thing is it seems like the llm was able to tell there was in an issue in both attempts that I have ran so far. GPT did mention something about pulling out the history, maybe implementing this will help me see what is going on\n",
    "> may want to double check the get_wiki call too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9f770ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No page found on Wikipedia for Hammond Louisiana'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wiki('Hammond Louisiana')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa60e8",
   "metadata": {},
   "source": [
    "looks like I may just have forgotten to import the library....\n",
    "<br>\n",
    "<br>\n",
    "even after importing it There is still an error finding the page it seems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "465c79a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_obj = wikipediaapi.Wikipedia(\n",
    "    user_agent='TopicSearch (grant.mlanier@gmail.com)',\n",
    "    language='en'\n",
    ")\n",
    "page = wiki_obj.page('Hammond, Louisiana')\n",
    "page.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2075c4",
   "metadata": {},
   "source": [
    "So it looks like there may need to be some formatting that would have needed to be done? I found that if I left the comma ',' from the City, State format the wiki page will not be found. Wonder if that is the case about other cities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aeeb12b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hammond, Louisiana, is a city in Jefferson Parish, located just north of New Orleans. Here‚Äôs a concise overview:  \n",
       "\n",
       "### **Key Highlights**  \n",
       "- **History**:  \n",
       "  - Founded in 1836, it was a stop on the Natchez Trace Parkway and played a role in the Civil War (e.g., the 1862 Battle of Hammond).  \n",
       "  - Known as the \"Cradle of the Blues\" for its jazz heritage and cultural influence.  \n",
       "\n",
       "- **Landmarks**:  \n",
       "  - **Jefferson Parish Courthouse** (1852, historic architecture).  \n",
       "  - **Hammond Park** (a National Historic Landmark with antebellum homes).  \n",
       "  - **St. Louis Cathedral** (part of the New Orleans metropolitan area).  \n",
       "\n",
       "- **Culture & Economy**:  \n",
       "  - A hub for Creole and French heritage, with vibrant music and cuisine.  \n",
       "  - Proximity to New Orleans offers easy access to cultural attractions.  \n",
       "\n",
       "- **Demographics**:  \n",
       "  - Population ~50,000 (as of recent estimates).  \n",
       "\n",
       "If you‚Äôd like deeper insights into specific aspects (e.g., history, architecture, or local events), feel free to ask! üòä"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gen_agent.ask('What can you tell me about Hammond, Louisiana?')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573efeb",
   "metadata": {},
   "source": [
    "Still got the parish wrong... maybe it is in the way I am asking the question? Perhaps I should ask it specifically about the parish since it seems to be getting that wrong more often than not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d209b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hammond, Louisiana, is located in **Jefferson Parish**, which is just north of New Orleans. The parish is part of the New Orleans metropolitan area and is known for its historical and cultural landmarks. üòä"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gen_agent.ask('What parish is Hammond, Louisiana in?')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1bfdf813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I‚Äôm unable to access Wikipedia or external tools at the moment, but I can share that **Hammond, Louisiana** is located in **Jefferson Parish**, just north of New Orleans. Let me know if you‚Äôd like details about its history, culture, or landmarks! üòä"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gen_agent.ask('Can you check wikipedia for Hammond, Louisiana?')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e986cf",
   "metadata": {},
   "source": [
    "Looks like it is still having issues making the connection to the wiki tool maybe? or perhaps it is still not passing the formatting correct? This sounds like something that could be a good opportunity for multi shot prompting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc8a1c",
   "metadata": {},
   "source": [
    "To make sure and for future issues below is a refactor of the Tool Agent code to include the option to call a history of the tools called as well as a function to specifically call the last used tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aef12bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatToolAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: \"LlmClient\",\n",
    "        tools: list,\n",
    "        tool_dispatch: dict,\n",
    "        system_prompt: str = \"You are a helpful assistant. Use tools when helpful.\",\n",
    "        max_steps: int = 8,\n",
    "        max_history_msgs: Optional[int] = 50,\n",
    "    ):\n",
    "        self.llm = llm\n",
    "        self.tools = tools\n",
    "        self.tool_dispatch = tool_dispatch\n",
    "        self.system_prompt = system_prompt\n",
    "        self.max_steps = max_steps\n",
    "        self.max_history_msgs = max_history_msgs\n",
    "\n",
    "        # Conversation state lives here\n",
    "        self.messages: list[dict] = []\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Start a fresh conversation.\"\"\"\n",
    "        self.messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "\n",
    "    def _trim_history(self):\n",
    "        \"\"\"\n",
    "        Keep system + last N messages.\n",
    "        (This is a simple heuristic; later you can summarize older turns.)\n",
    "        \"\"\"\n",
    "        if self.max_history_msgs is None:\n",
    "            return\n",
    "        # always keep the system message at index 0\n",
    "        if len(self.messages) > (1 + self.max_history_msgs):\n",
    "            self.messages = [self.messages[0]] + self.messages[-self.max_history_msgs :]\n",
    "\n",
    "    def ask(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        One user turn, but may require multiple internal steps if tools are called.\n",
    "        Conversation history is preserved across calls.\n",
    "        \"\"\"\n",
    "        # Add user message to ongoing history\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self._trim_history()\n",
    "\n",
    "        for _ in range(self.max_steps):\n",
    "            # 1) model call\n",
    "            message = self.llm.chat(messages=self.messages, tools=self.tools)\n",
    "\n",
    "            # append assistant message to history\n",
    "            self.messages.append(message.model_dump())\n",
    "            self._trim_history()\n",
    "\n",
    "            # 2) if no tool calls -> return assistant content\n",
    "            if not message.tool_calls:\n",
    "                return message.content or \"\"\n",
    "\n",
    "            # 3) execute each requested tool call\n",
    "            for call in message.tool_calls:\n",
    "                tool_name = call.function.name\n",
    "                raw_args = call.function.arguments or \"{}\"\n",
    "\n",
    "                try:\n",
    "                    args = json.loads(raw_args)\n",
    "                except json.JSONDecodeError:\n",
    "                    result = f\"ERROR: Invalid JSON arguments: {raw_args}\"\n",
    "                else:\n",
    "                    func = self.tool_dispatch.get(tool_name)\n",
    "                    if not func:\n",
    "                        result = f\"ERROR: Unknown tool '{tool_name}'\"\n",
    "                    else:\n",
    "                        try:\n",
    "                            result = func(**args)\n",
    "                        except Exception as e:\n",
    "                            result = f\"ERROR: Tool '{tool_name}' failed: {e}\"\n",
    "\n",
    "                # 4) feed tool result back\n",
    "                self.messages.append(\n",
    "                    {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": call.id,\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": str(result),\n",
    "                    }\n",
    "                )\n",
    "                self._trim_history()\n",
    "\n",
    "        return \"ERROR: Reached max tool steps without a final answer.\"\n",
    "    \n",
    "    # New additions to extract tool call info from conversation history\n",
    "    def get_tool_calls(self) -> list[dict]:\n",
    "        calls = []\n",
    "        pending = {}\n",
    "\n",
    "        for msg in self.messages:\n",
    "            role = msg.get('role')\n",
    "\n",
    "            if role == 'assistant' and msg.get(\"tool_calls\"):\n",
    "                for call in msg[\"tool_calls\"]:\n",
    "                    pending[call['id']] = {\n",
    "                        'tool': call['function']['name'],\n",
    "                        'arguments' : call['function'].get('arguments'),\n",
    "                        'result': None\n",
    "                    }\n",
    "            elif role == 'tool':\n",
    "                call_id = msg.get('tool_call_id')\n",
    "                if call_id in pending:\n",
    "                    pending[call_id]['result'] = msg.get('content')\n",
    "                    calls.append(pending.pop(call_id))\n",
    "\n",
    "        return calls\n",
    "    \n",
    "    def print_tool_calls(self) -> None:\n",
    "        calls = self.get_tool_calls()\n",
    "        if not calls:\n",
    "            print(\"No tool calls made.\")\n",
    "            return\n",
    "        for i, c in enumerate(calls, 1):\n",
    "            print(f\"\\nTool Call #{i}\")\n",
    "            print(f\"Tool: {c['tool']}\")\n",
    "            try:\n",
    "                args = json.loads(c['arguments']) if c['arguments'] else {}\n",
    "                print(' Arguments:')\n",
    "                for k, v in args.items():\n",
    "                    print (f\"  - {k}: {v}\")\n",
    "            except Exception:\n",
    "                print(f\"arguments: {c['arguments']}\")\n",
    "            print(f\" Result: \\n   {c['result']}\")\n",
    "    \n",
    "    def last_tool_call(self) -> dict | None:\n",
    "        calls = self.get_tool_calls()\n",
    "        return calls[-1] if calls else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af053905",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_agent = ChatToolAgent(llm, tools, tools_dispatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "548b05c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hammond, Louisiana, is the largest city in Tangipahoa Parish, located 45 miles east of Baton Rouge and 45 miles northwest of New Orleans. With a population of **21,359** as of 2020, it is home to **Southeastern Louisiana University** and serves as the principal city of the Hammond Metropolitan Statistical Area. It is also part of the **Baton Rouge-Hammond Combined Statistical Area**. Let me know if you'd like further details!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gen_agent.ask('Can you check wikipedia for Hammond, Louisiana?')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43de59",
   "metadata": {},
   "source": [
    "Right after I add that feature it seems to work!??! was it an attempt issue again? Additional runs came back accurate as well and even faster but I think this may be due to the chat history since I was running the same cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0b5981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c715f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hammond, Louisiana, is the largest city in Tangipahoa Parish, situated 45 miles east of Baton Rouge and 45 miles northwest of New Orleans. As of the 2020 population estimates, its population was **21,359**, up from **20,019** in the 2010 U.S. Census. \n",
       "\n",
       "The city is home to **Southeastern Louisiana University** and serves as the principal city of the Hammond Metropolitan Statistical Area. It is also part of the larger **Baton Rouge-Hammond Combined Statistical Area**, highlighting its regional significance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gen_agent.ask('Can you check wikipedia for Hammond, Louisiana?')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f225c140",
   "metadata": {},
   "source": [
    "Even after running the reset method it pulls the right info. This is a good sign I think! Not sure what the issue was previously? Maybe another history thing and when I refactored the class it basically reset it again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "deb09080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool Call #1\n",
      "Tool: get_wiki\n",
      " Arguments:\n",
      "  - topic: Hammond, Louisiana\n",
      " Result: \n",
      "   Hammond is the largest city in Tangipahoa Parish, Louisiana, United States, located 45 miles (72 km) east of Baton Rouge and 45 miles (72 km) northwest of New Orleans. Its population was 20,019 in the 2010 U.S. census, and 21,359 at the 2020 population estimates program.\n",
      "Hammond is home to Southeastern Louisiana University. It is the principal city of the Hammond metropolitan statistical area, which includes all of Tangipahoa Parish and is a part of the Baton Rouge-Hammond combined statistical area.\n"
     ]
    }
   ],
   "source": [
    "gen_agent.print_tool_calls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00732ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond is the largest city in Tangipahoa Parish, Louisiana, United States, located 45 miles (72 km) east of Baton Rouge and 45 miles (72 km) northwest of New Orleans. Its population was 20,019 in the 2010 U.S. census, and 21,359 at the 2020 population estimates program.\\nHammond is home to Southeastern Louisiana University. It is the principal city of the Hammond metropolitan statistical area, which includes all of Tangipahoa Parish and is a part of the Baton Rouge-Hammond combined statistical area.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "88ada16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"You are a helpful assistant.\\n\"\n",
    "    \"When you call tools, you MUST use their results to produce a final answer in your own words.\\n\"\n",
    "    \"Do NOT just repeat tool output verbatim. Summarize, explain, and cite key points.\\n\"\n",
    "    \"If a tool returns long text, extract only the relevant parts.\"\n",
    ")\n",
    "new_agent = ChatToolAgent(llm, tools, tools_dispatch, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d91528a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hammond, Louisiana, is located in **Tangipahoa Parish**. It is the largest city in the parish and serves as its principal city. The parish is situated in southern Louisiana, approximately 45 miles east of Baton Rouge and 45 miles northwest of New Orleans."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_rep = new_agent.ask('Can you tell me what parish Hammond, Louisiana is in?')\n",
    "display(Markdown(new_rep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f698c",
   "metadata": {},
   "source": [
    "The above prompt is recommended to help the model just not regurgitate the tools output. The final 'system prompt' will need to be tweaked eventually to include multi shot prompting as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b559a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-workspace (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
